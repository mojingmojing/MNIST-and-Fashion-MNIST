{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit recognition\n",
    "## Yijing Xiao\n",
    "\n",
    "## https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time I played with the MNIST dataset, in this post I will train a classifier to classifier images of digits into different digit classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the training data into two parts, 90% of the training data becomes the new training set, and 10% becomes the dev set. Training data is used to build the model, and dev set is to determine if the model we just trained is a good model. The reason why we need dev set is because the model may easily overfit on the training set, but it cannot perform well outside the domain of the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()\n",
    "image_size = data.iloc[:, 1:].values.shape[1]\n",
    "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "image_width\n",
    "num_data = data.shape[0]\n",
    "num_train_data = int(num_data * 0.9)\n",
    "train_data = data.iloc[:num_train_data]\n",
    "dev_data = data.iloc[num_train_data:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Convolutional Neural Network \n",
    "\n",
    "borrowed from https://github.com/pytorch/examples/blob/master/mnist/main.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote a function to evaluate the model. It breaks the dataset into batches, and make predictions based on batched data. We count and accumulate the number of correct predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def eval(model, data):\n",
    "    '''\n",
    "        args:\n",
    "            data: 42000 * 785 matrix\n",
    "            \n",
    "    '''\n",
    "    model.eval()\n",
    "    correct_count = 0.\n",
    "    total_count = 0.\n",
    "    for i in range(0, data.shape[0], BATCH_SIZE):\n",
    "        batch_data = data.iloc[i:i+BATCH_SIZE, :].values\n",
    "        x = batch_data[:, 1:] # 32 * 784\n",
    "        y = batch_data[:, 0] # 32\n",
    "        x = Variable(torch.from_numpy(x), volatile=True).float()\n",
    "        y = Variable(torch.from_numpy(y), volatile=True)\n",
    "        pred = model(x)\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        correct_count += torch.sum(torch.max(pred, 1)[1] == y).data[0]\n",
    "        total_count += batch_data.shape[0]\n",
    "    return correct_count, total_count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following neural network is copied from https://github.com/pytorch/examples/blob/master/mnist/main.py . \n",
    "It works surprisingly well on this particular image recognition task. I wonder if it also works this well on other tasks. I might write another blog post for other tasks in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a neural network, convolutional neural network\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "        network structure borrowed from https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, int(image_height), int(image_width)) # B * 1 * 28 * 28\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) # B * 10 * 12 * 12\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) # B * 20 * 4 * 4\n",
    "        x = x.view(-1, 320) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# instantiate the model\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start training using Adam Optimizer. The optimizer helps us reduce the loss function we defined on the correct label and our predicted labels, the goal is to reduce the loss as much as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev acc: 0.8745238095238095\n",
      "save the model\n",
      "dev acc: 0.9238095238095239\n",
      "save the model\n",
      "dev acc: 0.9116666666666666\n",
      "dev acc: 0.9397619047619048\n",
      "save the model\n",
      "dev acc: 0.9452380952380952\n",
      "save the model\n",
      "dev acc: 0.9435714285714286\n",
      "dev acc: 0.9514285714285714\n",
      "save the model\n",
      "dev acc: 0.9502380952380952\n",
      "dev acc: 0.9488095238095238\n",
      "dev acc: 0.9607142857142857\n",
      "save the model\n",
      "dev acc: 0.9592857142857143\n",
      "dev acc: 0.9647619047619047\n",
      "save the model\n",
      "dev acc: 0.959047619047619\n",
      "dev acc: 0.9666666666666667\n",
      "save the model\n",
      "dev acc: 0.9645238095238096\n",
      "dev acc: 0.9707142857142858\n",
      "save the model\n",
      "dev acc: 0.9685714285714285\n",
      "dev acc: 0.9726190476190476\n",
      "save the model\n",
      "dev acc: 0.9697619047619047\n",
      "dev acc: 0.9692857142857143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_acc = 0.\n",
    "# start training\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    for i in range(0, train_data.shape[0], BATCH_SIZE):\n",
    "        batch_data = train_data.iloc[i:i+BATCH_SIZE, :].values\n",
    "        x = batch_data[:, 1:] # 32 * 784\n",
    "        y = batch_data[:, 0] # 32\n",
    "        x = Variable(torch.from_numpy(x)).float()\n",
    "        y = Variable(torch.from_numpy(y))\n",
    "        pred = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.nll_loss(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    correct_count, total_count = eval(model, dev_data)\n",
    "    acc = correct_count / total_count\n",
    "    print(\"dev acc: {}\".format(acc))\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        print(\"save the model\")\n",
    "        torch.save(model.state_dict(), \"model.th\")\n",
    "    else:\n",
    "        learning_rate *= 0.8\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions! and write the prediction file to the submision file. This gives me over 96% of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "model.load_state_dict(torch.load(\"model.th\"))\n",
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    correct_count = 0.\n",
    "    total_count = 0.\n",
    "    all_preds = []\n",
    "    for i in range(0, data.shape[0], BATCH_SIZE):\n",
    "        batch_data = data.iloc[i:i+BATCH_SIZE, :].values\n",
    "        x = batch_data[:, :] # 32 * 784\n",
    "        x = Variable(torch.from_numpy(x), volatile=True).float()\n",
    "        pred = model(x) # B*10\n",
    "        pred = np.argmax(pred.data.numpy(), 1)\n",
    "        all_preds.append(pred[None, :])\n",
    "    all_preds = np.concatenate(all_preds, 0)\n",
    "    return all_preds\n",
    "all_preds = predict(model, test_data)\n",
    "all_preds = all_preds.reshape(-1)\n",
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"ImageId,Label\\n\")\n",
    "    for i in range(all_preds.shape[0]):\n",
    "        f.write(\"{},{}\\n\".format(i+1, all_preds[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
